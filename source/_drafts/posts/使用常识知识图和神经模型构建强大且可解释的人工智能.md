---
date: 2024-07-22T17:00:42.954Z
updated: null
title: 使用常识知识图和神经模型构建强大且可解释的人工智能
slug: '5462435813'
oid: 669e903a2a6fe84dfe920698
categories: 人工智能
type: post
permalink: /posts/人工智能/5462435813
---


# Chapter9：Building Robust and Explainable AI with Commonsense Knowledge Graphs and Neural Models

## 总览

- **常识推理的背景和挑战**：作者回顾了早期的基于逻辑和知识库的常识推理方法，以及近年来的基于神经模型的方法，指出了它们的优缺点和局限性，提出了构建健壮和可解释的常识推理系统的开放挑战。
- **常识知识图谱的组织**：作者介绍了他们开发的常识知识图谱（CSKG），它是一个将七个不同来源的常识知识整合在一起的超关系图，包括概念、词汇、视觉、过程等方面的知识。作者还介绍了他们如何将常识知识划分为13个维度，以便于分析和利用。
- **基于知识的健壮常识推理**：作者介绍了他们的零样本问答框架，它利用CSKG生成自然语言的问题-答案对，用于预训练神经模型，然后在多个常识问答任务上进行零样本评估。作者还介绍了他们对神经模型的泛化能力、知识容量和迁移能力的研究。
- **基于知识的可解释常识推理**：作者介绍了他们的一些神经-符号方法，它们能够利用常识知识生成故事或问题的解释，例如基于路径生成器的方法、基于想象和语言化的方法、以及基于过程理解的方法。作者还讨论了如何设计具有本质可解释性的系统，例如通过选择预训练的知识维度、生成场景图、或者学习生成知识路径。

## 作者

**Filip Ilievski**, Information Sciences Institute, University of Southern California 来自南加州大学信息科学研究所；
**Kaixin Ma**, Language Technologies Institute, Carnegie Mellon University 来自卡内基梅隆大学语言技术研究所；
**Alessandro Oltramari**, Bosch Research and Technology Center & Bosch Center for Artificial Intelligence, Pittsburgh 来自博世研究与技术中心和博世人工智能中心，位于匹兹堡；
Peifeng Wang, Department of Computer Science, University of Southern California 来自南加州大学计算机科学系；
Jay Pujara, Information Sciences Institute, University of Southern California 来自南加州大学信息科学研究所。

## 摘要  

常识推理是神经符号技术的一个有吸引力的测试平台，因为它是一个纯神经和符号方法难以解决的困难挑战。在本章中，我们回顾了结合大规模知识资源和通用神经模型的常识推理方法，以实现鲁棒性和可解释性。我们讨论了知识表示和整合的努力，以协调异构的知识。我们介绍了一些代表性的神经符号常识方法，利用这些常识知识来对问题和故事进行推理。推理机制的范围包括程序化推理、类比推理和想象推理。我们讨论了设计具有本地可解释性的系统的不同策略，例如工程化预训练的知识维度，生成场景图，以及学习生成知识路径。

## 1.1 介绍

几乎任何情境都可以与一组常识问题或需要**智能体**（下译为代理）判断和解释其合理性的叙事相关联。想象一种能够协助孩子在家中日常生活的人工智能。如果孩子感到寒冷，理解人类心理的人工智能代理会推理出感冒是一种不舒服的状态，而且当人们感到寒冷时，他们通常会努力变暖。基于对物体属性和事件因果关系的理解，代理会建议孩子穿上毯子变暖。在意识到家里没有毯子后，代理会建议孩子可以穿上夹克，因为它的常识知识告诉它，夹克和毯子是不同的物体，都可以用来保暖，而且都可以在家里找到。以这样无缝的方式协助人类，需要人工智能能够可靠地表现出常识。

这种常识推理是神经符号技术的一个有吸引力的测试平台，因为它是一个纯神经和符号方法难以解决的挑战。神经（语言）模型是通过从叙事数据中学习模式来成功解决基准测试的，但这是以牺牲可解释性为代价的。常识知识和公理是可用的，但它们不能单独推广到新的情境和捕捉任意的上下文变化。**现有的神经符号推理器显示了将神经模型与常识知识相结合的潜力**，但这是在强烈假设每个新的基准测试都有训练数据的前提下的。最近对常识推理的可扩展评估表明，神经方法和知识库对提高模型的准确性有积极的影响。然而，准确性的提高并没有伴随着对常识的感知的显著提高，因为同样的模型是脆弱的，几乎没有为它们的推理提供理由，也不能超越训练分布进行推广。  

在本章中，我们回顾了我们在常识推理方法方面的工作，这些方法将大规模的知识资源与可推广的神经模型相结合，以实现鲁棒性和可解释性。我们致力于构建能够在开放世界环境中展示其常识推理能力的任务，如问答和故事理解。我们的神经符号方法依赖于三个关键的推动力：
	1）组织异构的常识知识源，
	2）利用这些知识支持零样本评估环境下的鲁棒神经推理，
	3）设计具有原生可解释性的神经符号方法。

**组织常识性知识（第1.3节）** 在常识性知识源的异质性和需要联合使用的指导下，我们开发了常识性知识图（CSKG）：一个整合了七个常识性知识源的“超图”。CSKG包括常识性知识源，如ConceptNet ，词汇资源，如WordNet [3]，和视觉资源，如Visual Genome [4]。CSKG还包含了通用领域的Wikidata图的一个子集，我们通过形式化常识性原则来提取它[5]。在[6]中，我们将CSKG和其他十几个知识源的关系分为13个知识维度（例如，空间维度）。

**使用知识构建强大的AI代理（第1.4节）** 观察到语言模型在常识性任务上的泛化能力依赖于训练数据的表面特征，如词汇[7, 8]，我们专注于通过在零样本设置下适应常识性代理来回答问题。我们利用CSKG生成自然语言的问题-答案对，用于预训练神经模型，在常识性问答基准上观察到了稳定的性能[9]。我们使用CSKG的不同维度进行零样本预训练的实验[6]表明，某些知识维度（例如，时间知识）对于当前的语言模型来说更具有信息性和有用性。进一步的研究揭示了知识采样和大小、语言建模架构和任务之间的关键相互作用模式[10]。

**设计可解释的人工智能代理（第1.5节）** 神经符号方法有潜力提供对其推理过程的可理解的洞察，但是，没有基于常识知识解释决策的标准方法。我们开发了一种方法，可以用可解释的路径生成器（PG）[11]增强常识问答系统。PG结合了神经语言模型和常识知识图，将问题中的概念与答案中的概念通过动态的、可能是新颖的多跳关系路径连接起来，作为支持或反驳答案的常识解释。我们的后续工作Imagine-and-Verbalize方法[12]能够根据初始的上下文和提示关键词生成或完成故事，首先想象关键词之间的相互作用作为一个场景知识图。我们还开发了一种程序理解方法，CGLI [13]，它可以通过跟踪实体状态并对其进行推理来检测叙事中的冲突。

## 1.2  背景

- 将常识融入机器一直是人工智能领域的一个臭名昭著的挑战[14]。基于数学和逻辑的早期常识推理是理论性的，与实践的联系很少[15]。Minsky的框架[16]将新遇到的情境映射到已知的结构。一个框架，例如，儿童生日聚会，包括了如何使用框架、接下来可能发生的事情以及当期望被违反时该怎么做的信息。Scripts[17]是针对特定上下文中事件序列的框架规范。同时，也有一些理论试图处理类比推理[18, 19]、基于案例的推理[20]、朴素物理[21, 22, 23]、时间[24]和朴素心理学[25]。基于知识的系统CYC[26]包含了关于数十种一般情境和许多特定情境的陈述和假设，称为微理论。CYC是专有的，但是它的一部分已经向研究人员开放，促进了诸如网络查询扩展和优化[27]、段落检索和演绎问答[28]以及情境跟踪[29]等应用的发展。CYC参与了Companion认知架构[30]的构建，该架构力图捕捉类比和定性推理，并已经证明对教科书问题解决、道德决策和常识推理任务有用。应用公理化理论和纯粹的基于知识的系统来解释新的情境和任意的情境变化是一项开放的挑战。将常识公理与常识知识和神经模型相结合，以实现对新情境的泛化，也是一项开放的挑战。

- 大型的（transformer）语言模型（LM），如BERT[31]，RoBERTa [32]，GPT-2 [33]和GPT-3 [34]，已经被证明在评估机器常识的多项选择QA基准上表现良好，例如SocialIQA [35]和PhysicalIQA [36]。在纯神经LM方法中，通常将预训练的LM针对常识基准进行微调，并在预训练模型之上放置一个任务特定的预测层。虽然这些模型能够适应几乎任何基准，但它们缺乏解释其决策的机制，并且已经被证明经常基于偶然的相关性来解决基准，这限制了它们在开放世界任务中的应用[37, 38, 39]。神经（语言）模型应该与常识知识和公理相结合，以便促进对开放世界场景的可解释性和可靠性的应用。

- 近期的工作[40, 37]表明，**将神经方法与来自ConceptNet [2]，WordNet [41]和ATOMIC [42]的结构化背景知识相结合，可以提高在部分由这些资源衍生的常识数据集上的准确性。例如，将结构化的知识，形式化为针对任务的词汇化证据路径，可以注入到语言模型中[44, 45]。或者，可以使用图和关系网络来评分答案候选，通过语言模型的知识来指导图的结构[46, 47]。完整的知识图也可以直接在训练中加入，通过引入额外的建模目标，来教授模型关于通用常识的知识，而不管具体的任务是什么[48, 49, 50, 51, 52]。** 这样的神经符号方法的效用依赖于一个强假设，即有特定于基准的训练数据可用，而且知识的分布与任务相似。在我们的工作中，我们通过基于故事语料库和常识资源生成有意义的数据来预训练模型，来减轻对训练数据的依赖。之前的工作也尝试通过询问澄清提示来从语言模型或知识图中获取知识[53]，或者使用在常识知识图上训练的生成式链接预测模型[54, 55]，来减轻对训练数据的依赖。这样的零样本方法减轻了对特定于基准的训练数据的需求，但它们缺乏解释其决策的能力。我们展示了如何利用常识知识和公理来丰富故事的解释，并探索了可以从生成的数据中学习的技术。

- 近期，一些朝着可解释模型的方向发展的工作，例如WT5 [56]，它是基于生成式语言模型T5 [57]的微调版本，显示出了在不侧重常识推理的基准测试上解释决策的潜力。我们将尝试使用T5来基于常识知识和公理来解释故事。一些基于图的方法，例如SalKG [58]，能够学习知识图中的显著性指标，但是这些方法依赖于训练数据，并且缺乏更深层次的因果和心理解释。我们展示了如何使用常识知识和故事来预训练模型，以生成开放世界故事的解释。

- 总之，以往的常识推理工作已经产生了丰富的常识模型、知识资源和公理理论。然而，构建能够在没有特定基准训练数据的情况下（鲁棒性）跨任务表现良好，并且能够为其决策提供常识解释（可解释性）的AI代理仍然是一个开放的挑战。

## 1.3  组织常识性知识

## 1.4  稳健的常识推理

- 只要训练数据可用，神经模型就可以有效地适应一系列常识推理任务。 然而，这些模型已被证明在其他数据集上的性能要低得多，即使对于类似类型的推理也是如此，这表明它们在不解决任务的情况下学习了捷径并解决了数据集。对未见过的数据集的泛化至关重要，因为依赖于特定于基准的训练数据始终可用的期望是不切实际的。 为了缓解这个问题，我们专注于零样本评估，即模型训练一次并跨基准进行评估，而无需访问任何特定于基准的训练数据。 我们的方法依赖于我们按照上一节所述组织的大型常识知识源 (CSKG)。

### 1.4.1  模型稳健性研究

- 解决这些常识推理任务的最广泛采用的方法是通过在任务特定的训练数据上微调大型预训练语言模型[31, 32]。已经证明，语言模型能够在大规模文本数据的预训练过程中获取一定的常识背景知识[65, 66, 67]。这种做法的缺点是，微调可能导致模型过拟合任务特定的数据，从而忘记了预训练过程中获得的知识。而且，期望模型能够访问特定于基准测试的数据在现实世界的设置中是不现实的，这使得微调模型的实用性受到质疑。鉴于这些发现和这些语言模型的大容量，最近的工作提出了一些微调LM的轻量级替代方案，例如，只更新少量的额外参数[68, 69]，或者在保持模型权重不变的同时更新输入[70, 71]。直观地说，这些轻量级的方法可能在很大程度上保留模型的预训练知识，并为目标任务引出合适的知识，前提是这些知识已经在模型参数中编码了。

- 这就引出了一个自然的问题：模型从常识推理数据集中学到了什么？不同的适应策略在准确性和泛化性方面的表现如何？为了回答这些问题，我们研究了[72]三种有代表性的学习方法：常规的微调，模型扩展的前缀调整[69]，和模型提示的自动提示[71]。我们将它们应用到两种有代表性的模型类别上：自回归语言模型GPT-2 [33]和序列到序列语言模型BART [73]，并在生成性基准ProtoQA [74]和CommonGen [75]上进行评估。)我们的实验表明，微调的表现最好，它能够同时学习任务的内容和结构，但是也容易过拟合，而且对新颖答案的泛化能力有限。提示方法的准确性较低，但是对对抗性划分的鲁棒性较高，它们能够在训练和测试数据之间没有答案重叠的情况下保持准确性。通过前缀调整扩展模型在任务准确性、泛化性和鲁棒性之间达到了一个“最佳点”。

- 我们注意到，有些答案可能是出于错误的原因而正确的。例如，对于一个训练问题“*人们喜欢蒸的蔬菜是什么*”，模型学习到的答案是花椰菜，这恰好也是开发问题“*和你的头一样大的蔬菜是什么*”的正确答案。为了进一步研究这一现象，我们创建了 ProtoQA 的手动子集，其中我们否定了 30 个原始问题，期望模型提供一组不同的答案。 然而，我们观察到大部分模型答案在原始问题和否定问题之间是共享的（表 6）。

- 在另一项工作中[76]，我们研究了LM对常识推理的三个方面：知识容量、可迁移性和归纳能力。为了衡量LM的知识容量能力，我们考察了LM是否能够同时适应多个CKG，并在每个CSKG上进行测试。我们通过评估LM在多个源CSKG上的初始适应是否能够减少进一步适应新CSKG的努力来测试它们的可迁移性。LM的归纳能力是通过改变训练集和测试集中对象的重叠程度来衡量的。我们发现LM能够同时推理多个CKG，而不损失目标推理任务的性能，但是知识在任务之间的可迁移性有限。LM对常识推理的归纳能力在很大程度上依赖于训练过程中是否观察到对象，再次表明模型在很大程度上依赖于训练数据的特征。

### 1.4.2  用于问答的零样本框架

## 1.5  可解释的常识推理

## 1.6  结束语

本章总结了我们建立具有常识的鲁棒和可解释的人工智能代理的追求。鲁棒性和可解释性是我们可以信赖的人工智能代理和社会可以大规模采用的必要维度。我们证明了大型语言模型的泛化能力可以与知识图和结构化表示相结合，实现两全其美。由此产生的模型能够在未见过的数据集和任务上表现合理，提取任务的中间和隐含特征（如实体状态或潜在概念），并做出可以透明地追溯到模型推理模式的决策。然而，通往值得信赖的通用领域代理的旅程还很漫长。类比推理是一个强大的解释和泛化机制的关键，它得到了认知科学工作的支持，并且最近被我们和其他人在人工智能领域进行了探索。将现有的神经符号方法与常识理论（例如，心理公理或情感）更好地结合，可能会进一步提高方法的泛化性和可解释性。最后，代表性评估任务和数据集的设计需要伴随这些发展：需要越来越复杂的基准来激励和衡量新的推理方法的进步。