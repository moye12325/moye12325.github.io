import os
import re
from pathlib import Path
from collections import defaultdict

# è¯¦ç»†çš„åˆ†ç±»è§„åˆ™å’Œå…³é”®è¯
CATEGORY_KEYWORDS = {
    'æ·±åº¦å­¦ä¹ ': {
        'keywords': [
            'æ·±åº¦å­¦ä¹ ', 'ææ²', 'ç¥ç»ç½‘ç»œ', 'CNN', 'RNN', 'LSTM', 'GRU', 'Transformer',
            'BERT', 'ResNet', 'AlexNet', 'NiN', 'seq2seq', 'æ³¨æ„åŠ›', 'Attention',
            'å·ç§¯', 'æ± åŒ–', 'ä¼˜åŒ–ç®—æ³•', 'BatchSize', 'æ¢¯åº¦', 'åå‘ä¼ æ’­', 'å‰å‘ä¼ æ’­',
            'æ‰¹é‡è§„èŒƒåŒ–', 'Dropout', 'æƒé‡è¡°é€€', 'FCN', 'R-CNN', 'YOLO', 'SSD',
            'è¯­ä¹‰åˆ†å‰²', 'ç›®æ ‡æ£€æµ‹', 'å›¾åƒåˆ†ç±»', 'æ ·å¼è¿ç§»', 'å¾®è°ƒ'
        ],
        'weight': 1.5
    },
    'æœºå™¨å­¦ä¹ ': {
        'keywords': [
            'æœºå™¨å­¦ä¹ ', 'æå®æ¯…', 'GAN', 'Domain Adaptation', 'Life long learning',
            'å¼ºåŒ–å­¦ä¹ ', 'Reinforcement Learning', 'Explainable AI', 'NLP',
            'è‡ªç›‘ç£', 'Self-supervised', 'Homework', 'HW'
        ],
        'weight': 1.3
    },
    'æ•°æ®ç»“æ„ä¸ç®—æ³•': {
        'keywords': [
            'ç¨€ç–æ•°ç»„', 'é˜Ÿåˆ—', 'é“¾è¡¨', 'æ ˆ', 'é€’å½’', 'æ’åº', 'æŸ¥æ‰¾', 'å“ˆå¸Œ',
            'HashTable', 'HashMap', 'æ³¢å…°å¼', 'ä¸­ç¼€', 'åç¼€', 'LeetCode',
            'ç®—æ³•', 'æ•°æ®ç»“æ„', 'äºŒå‰æ ‘', 'å›¾', 'åŠ¨æ€è§„åˆ’', 'è´ªå¿ƒ', 'å›æº¯',
            'æ»‘åŠ¨çª—å£', 'åŒæŒ‡é’ˆ', 'å¿«æ…¢æŒ‡é’ˆ'
        ],
        'weight': 1.2
    },
    'Pythonå¼€å‘': {
        'keywords': [
            'Python', 'python', 'conda', 'pip', 'æ¨¡å—', 'setup.py', '.pth',
            'pyproject.toml', 'requirements.txt', 'å¤šçº¿ç¨‹', 'å¤šè¿›ç¨‹', 'åç¨‹',
            'asyncio', 'PyTorch', 'TensorFlow', 'numpy', 'pandas'
        ],
        'weight': 1.0
    },
    'Javaå¼€å‘': {
        'keywords': [
            'Java', 'java', 'JVM', 'å¹¶å‘', 'é›†åˆ', 'æ•°æ®ç±»å‹', 'ä½è¿ç®—',
            'Spring', 'SpringBoot', 'åŒäº²å§”æ´¾'
        ],
        'weight': 1.0
    },
    'Webå¼€å‘': {
        'keywords': [
            'nginx', 'Nginx', 'MyBatis', 'RabbitMQ', 'JumpServer', 'éƒ¨ç½²',
            'ç«¯å£è½¬å‘', '403', '404', 'æœåŠ¡å™¨', 'server', 'MySQL', 'Redis',
            'MVCC', 'æ•°æ®åº“', 'å‰åç«¯åˆ†ç¦»'
        ],
        'weight': 1.0
    },
    'å¼€å‘å·¥å…·': {
        'keywords': [
            'VSCode', 'vscode', 'Git', 'GitHub', 'Navicat', 'Docker',
            'docker', 'WSL', 'Windows Server', 'sftp', 'ZeroMQ'
        ],
        'weight': 1.0
    },
    'æµ‹è¯•': {
        'keywords': [
            'æµ‹è¯•', 'Test', 'testing', 'ç¼ºé™·', 'æ¥å£æµ‹è¯•', 'Software Testing',
            'Test Design'
        ],
        'weight': 1.0
    },
    'é¢è¯•ç¬”è¯•': {
        'keywords': [
            'æ ¡æ‹›', 'ç¬”è¯•', 'é¢è¯•', 'ç¼–ç¨‹é¢˜', '360', 'ç™¾åº¦', 'ç¾å›¢', 'å°ç±³',
            'è¥¿å±±å±…', 'ç±³å“ˆæ¸¸', 'å»å“ªå„¿'
        ],
        'weight': 1.0
    },
    'é‡å­è®¡ç®—': {
        'keywords': [
            'Llama', 'LLM', 'å¤§æ¨¡å‹', 'ç¥ç»ç¬¦å·', 'çŸ¥è¯†å›¾', 'é‡å­é—¨', 'é‡å­'
        ],
        'weight': 1.0
    },
    'éšç¬”': {
        'keywords': [
            'å…±é¸£', 'è®°å½•', 'å½’æ ¡', 'ç¾å…ƒæ½®æ±', 'ä¼¸æ‰‹å…š', 'è¯æ€§å¯¹ç…§è¡¨',
            'æ¯”èµ›è®°å½•', 'resume', 'reasoning'
        ],
        'weight': 0.8
    }
}

# æ ‡ç­¾æå–è§„åˆ™
TAG_RULES = {
    # ç¼–ç¨‹è¯­è¨€
    'Python': ['python', 'Python', '.py', 'pip', 'conda'],
    'Java': ['java', 'Java', '.java', 'JVM'],
    'JavaScript': ['javascript', 'js', 'JavaScript', 'JS'],
    
    # æ·±åº¦å­¦ä¹ æ¡†æ¶
    'PyTorch': ['pytorch', 'PyTorch', 'torch'],
    'TensorFlow': ['tensorflow', 'TensorFlow'],
    
    # æ·±åº¦å­¦ä¹ ç›¸å…³
    'æ·±åº¦å­¦ä¹ ': ['æ·±åº¦å­¦ä¹ ', 'Deep Learning', 'DL'],
    'æœºå™¨å­¦ä¹ ': ['æœºå™¨å­¦ä¹ ', 'Machine Learning', 'ML'],
    'ç¥ç»ç½‘ç»œ': ['ç¥ç»ç½‘ç»œ', 'Neural Network', 'CNN', 'RNN'],
    'Transformer': ['Transformer', 'transformer', 'BERT', 'GPT'],
    'è®¡ç®—æœºè§†è§‰': ['CV', 'å›¾åƒ', 'ç›®æ ‡æ£€æµ‹', 'è¯­ä¹‰åˆ†å‰²', 'å›¾åƒåˆ†ç±»'],
    'NLP': ['NLP', 'è‡ªç„¶è¯­è¨€', 'Natural Language'],
    
    # æ•°æ®ç»“æ„ä¸ç®—æ³•
    'ç®—æ³•': ['ç®—æ³•', 'Algorithm', 'algorithm'],
    'æ•°æ®ç»“æ„': ['æ•°æ®ç»“æ„', 'Data Structure', 'é“¾è¡¨', 'æ ˆ', 'é˜Ÿåˆ—', 'æ ‘'],
    'LeetCode': ['LeetCode', 'leetcode', 'Leetcode'],
    
    # Webå¼€å‘
    'Nginx': ['nginx', 'Nginx'],
    'MyBatis': ['mybatis', 'MyBatis'],
    'Spring': ['spring', 'Spring', 'SpringBoot'],
    'Redis': ['redis', 'Redis'],
    'MySQL': ['mysql', 'MySQL', 'sql', 'SQL'],
    'Docker': ['docker', 'Docker', 'å®¹å™¨'],
    
    # å·¥å…·
    'Git': ['git', 'Git', 'GitHub', 'github'],
    'VSCode': ['vscode', 'VSCode', 'VS Code'],
    
    # å…¶ä»–
    'éƒ¨ç½²': ['éƒ¨ç½²', 'deploy', 'deployment'],
    'æµ‹è¯•': ['æµ‹è¯•', 'test', 'Test', 'Testing'],
    'é¢è¯•': ['é¢è¯•', 'ç¬”è¯•', 'interview'],
    'æ€§èƒ½ä¼˜åŒ–': ['ä¼˜åŒ–', 'æ€§èƒ½', 'performance', 'optimization'],
}

def analyze_content(title, content):
    """æ·±åº¦åˆ†ææ–‡ç« å†…å®¹"""
    # åˆå¹¶æ ‡é¢˜å’Œå†…å®¹å‰2000å­—ç¬¦
    text = f"{title} " * 3 + content[:2000]  # æ ‡é¢˜æƒé‡æ›´é«˜
    text_lower = text.lower()
    
    # è®¡ç®—æ¯ä¸ªåˆ†ç±»çš„å¾—åˆ†
    category_scores = defaultdict(float)
    
    for category, config in CATEGORY_KEYWORDS.items():
        score = 0
        for keyword in config['keywords']:
            # å…³é”®è¯åŒ¹é…æ¬¡æ•°
            count = text.count(keyword)
            if count > 0:
                score += count * config['weight']
        
        if score > 0:
            category_scores[category] = score
    
    # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„åˆ†ç±»
    if category_scores:
        best_category = max(category_scores, key=category_scores.get)
        return best_category
    
    return 'å…¶ä»–'

def extract_tags(title, content):
    """æå–æ ‡ç­¾"""
    text = title + ' ' + content[:1500]
    tags = set()
    
    for tag, keywords in TAG_RULES.items():
        for keyword in keywords:
            if keyword in text:
                tags.add(tag)
                break
    
    # é™åˆ¶æ ‡ç­¾æ•°é‡
    return sorted(list(tags))[:6]

def update_article_metadata(filepath):
    """æ›´æ–°å•ç¯‡æ–‡ç« çš„å…ƒæ•°æ®"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"âŒ è¯»å–å¤±è´¥ {filepath.name}: {e}")
        return False
    
    # æå–front matter
    match = re.match(r'^---\n(.*?)\n---\n(.*)$', content, re.DOTALL)
    if not match:
        print(f"âš ï¸  è·³è¿‡ {filepath.name}: æ²¡æœ‰front matter")
        return False
    
    frontmatter = match.group(1)
    body = match.group(2)
    
    # æå–æ ‡é¢˜
    title_match = re.search(r'title:\s*(.+)', frontmatter)
    if not title_match:
        print(f"âš ï¸  è·³è¿‡ {filepath.name}: æ²¡æœ‰æ ‡é¢˜")
        return False
    
    title = title_match.group(1).strip()
    
    # æå–æ—¥æœŸ
    date_match = re.search(r'date:\s*(.+)', frontmatter)
    date = date_match.group(1).strip() if date_match else '2024-07-22 00:00:00'
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¯†ç ä¿æŠ¤
    password_match = re.search(r'password:\s*(.+)', frontmatter)
    password = password_match.group(1).strip() if password_match else None
    
    # åˆ†æå†…å®¹
    category = analyze_content(title, body)
    tags = extract_tags(title, body)
    
    # æ„å»ºæ–°çš„front matter
    new_frontmatter = f"""---
title: {title}
date: {date}
categories: [{category}]
tags: {tags}"""
    
    if password:
        new_frontmatter += f"\npassword: {password}"
    
    new_frontmatter += "\n---"
    
    new_content = new_frontmatter + '\n' + body
    
    # å†™å›æ–‡ä»¶
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(new_content)
        print(f"âœ… {filepath.name}")
        print(f"   åˆ†ç±»: {category} | æ ‡ç­¾: {tags}")
        return True
    except Exception as e:
        print(f"âŒ å†™å…¥å¤±è´¥ {filepath.name}: {e}")
        return False

def main():
    posts_dir = Path('source/_posts')
    
    if not posts_dir.exists():
        print("âŒ é”™è¯¯: source/_posts ç›®å½•ä¸å­˜åœ¨")
        return
    
    md_files = sorted(posts_dir.glob('*.md'))
    print(f"ğŸ“š æ‰¾åˆ° {len(md_files)} ç¯‡æ–‡ç« \n")
    
    # ç»Ÿè®¡
    stats = defaultdict(int)
    updated = 0
    
    for filepath in md_files:
        if filepath.name == 'hello-world.md':
            continue
        
        if update_article_metadata(filepath):
            updated += 1
    
    print(f"\nâœ¨ å®Œæˆ! æˆåŠŸæ›´æ–° {updated}/{len(md_files)} ç¯‡æ–‡ç« ")

if __name__ == '__main__':
    main()
